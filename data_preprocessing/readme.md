# Data preprocessing

## SRS to DNA

### Scope

The `dna_from_srs.py` script extracts DNA sequences for all OTUs (Operational Taxonomic Units) associated with a given SRS (Sequence Read Sample) ID. 

It performs a two-step lookup process:
1. **SRS ID → OTU IDs**: Maps a sample ID to all its associated OTUs using `samples-otus-97.parquet`
2. **OTU IDs → DNA sequences**: Retrieves the DNA sequence for each OTU using `otus_97_to_dna.parquet`

### Prerequisites

Before running the script, you need to download the required parquet files:

1. Download the parquet files from this [public Google Drive folder](https://drive.google.com/drive/folders/1pB1tmbkAr_gIY-Cyec6lOuMWUgk8rLdq?usp=sharing)
   - `samples-otus-97.parquet` (2.22 GB)
   - `otus_97_to_dna.parquet` (75 MB)
2. Place both files in the `mapref_data/` folder

### Usage

```bash
uv run dna_from_srs.py --srs_id <SRS_ID>
```

### Inputs

- **SRS ID** (via command line argument): The sample identifier to extract sequences for
- **mapref_data/samples-otus-97.parquet**: Mapping between SRS IDs and OTU IDs
- **mapref_data/otus_97_to_dna.parquet**: Mapping between OTU IDs and DNA sequences

### Outputs

- **CSV file**: Saved to `dna_sequences/<SRS_ID>.csv` containing:
  - `otu_id`: The OTU identifier
  - `dna_sequence`: The corresponding DNA sequence

### Example

```bash
uv run dna_from_srs.py --srs_id SRS7011253
```

This will create `dna_sequences/SRS7011253.csv` with all OTU-DNA mappings for that sample.

### Implementation Details

The script uses PyArrow for efficient parquet file filtering and pandas for data manipulation. Progress is displayed via tqdm for each OTU being processed.

## Generate DNA Embeddings

### Scope

The `generate_dna_embeddings.py` script converts DNA sequences into vector embeddings using the ProkBERT transformer model. This step transforms the raw DNA sequences (generated by `dna_from_srs.py`) into numerical representations that can be used for machine learning tasks.

The script:
1. Reads all CSV files from the `dna_sequences/` directory
2. Processes DNA sequences in batches using the ProkBERT model
3. Generates 384-dimensional embeddings for each OTU
4. Stores embeddings in a hierarchical HDF5 file structure: `SRS_ID/OTU_ID → embedding vector`

### Prerequisites

- Python packages: `transformers`, `torch`, `h5py`, `pandas`, `numpy`, `tqdm`
- DNA sequence CSV files generated by `dna_from_srs.py` in the `dna_sequences/` folder
- Sufficient memory for loading the ProkBERT model (adjust batch size if needed)

### Usage

```bash
python generate_dna_embeddings.py
```

### Configuration

You can modify the following parameters at the top of the script:

- `DNA_SEQUENCES_DIR`: Input directory containing SRS CSV files (default: `"dna_sequences"`)
- `OUTPUT_PATH`: Output HDF5 file path (default: `"dna_embeddings/prokbert_embeddings.h5"`)
- `BATCH_SIZE`: Number of sequences to process at once (default: `32`)
- `MODEL_NAME`: ProkBERT model variant (default: `"neuralbioinfo/prokbert-mini-long"`)
- `DEVICE`: Computing device - `"cpu"`, `"cuda"`, or `"mps"` (default: `"cpu"`)

### Inputs

- **dna_sequences/*.csv**: CSV files containing OTU-DNA mappings (output from `dna_from_srs.py`)

### Outputs

- **dna_embeddings/prokbert_embeddings.h5**: HDF5 file with hierarchical structure
  - Each SRS ID is a group
  - Each OTU ID is a dataset containing a 384-dimensional embedding vector

### Example

After running the script, you can inspect the output:

```python
import h5py

with h5py.File('dna_embeddings/prokbert_embeddings.h5', 'r') as f:
    # Access embedding for a specific OTU in a specific sample
    embedding = f['SRS7011253']['OTU_12345'][:]
    print(embedding.shape)  # (384,)
```

### Implementation Details

The script uses the ProkBERT transformer model for generating contextualized DNA embeddings. Sequences are processed in batches for memory efficiency, and mean pooling is applied over the sequence length to produce fixed-size embeddings. Progress is displayed via tqdm for both SRS samples and individual batches.

